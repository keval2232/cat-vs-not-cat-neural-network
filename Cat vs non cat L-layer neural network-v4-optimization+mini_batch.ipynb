{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-excellence",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-steel",
   "metadata": {},
   "source": [
    "Begin by importing all the packages you'll need during this assignment. \n",
    "\n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- `dnn_app_utils` provides the functions implemented in the \"Building your Deep Neural Network: Step by Step\" assignment to this notebook.\n",
    "- `np.random.seed(1)` is used to keep all the random function calls consistent. It helps grade your work - so please don't change it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "desirable-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keval_nn import *\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-front",
   "metadata": {},
   "source": [
    "<h3> 3.1 data loading </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Example of a data\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-college",
   "metadata": {},
   "source": [
    "As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.\n",
    "\n",
    "<img src=\"imvectorkiank.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Image to vector conversion.</font></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-bulgarian",
   "metadata": {},
   "source": [
    "Now that you're familiar with the dataset, it's time to build a deep neural network to distinguish cat images from non-cat images!\n",
    "\n",
    "You're going to build two different models:\n",
    "\n",
    "- A 2-layer neural network\n",
    "- An L-layer deep neural network\n",
    "\n",
    "Then, you'll compare the performance of these models, and try out some different values for $L$. \n",
    "\n",
    "Let's look at the two architectures:\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: 2-layer neural network. <br> The model can be summarized as: INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT.</font></center></caption>\n",
    "\n",
    "<u><b>Detailed Architecture of Figure 2</b></u>:\n",
    "- The input is a (64,64,3) image which is flattened to a vector of size $(12288,1)$. \n",
    "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- Then, add a bias term and take its relu to get the following vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n",
    "- Repeat the same process.\n",
    "- Multiply the resulting vector by $W^{[2]}$ and add the intercept (bias). \n",
    "- Finally, take the sigmoid of the result. If it's greater than 0.5, classify it as a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "rental-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 3,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-surveillance",
   "metadata": {},
   "source": [
    "<font size=70px color='orange'>Mini batch </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "imposed-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(12/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "apparent-mayor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "shape of the 1st mini_batch_X: (12288, 64)\n",
      "shape of the 2nd mini_batch_X: (12288, 64)\n",
      "shape of the 3rd mini_batch_X: (12288, 64)\n",
      "shape of the 4th mini_batch_X: (12288, 17)\n",
      "shape of the 1st mini_batch_Y: (1, 64)\n",
      "shape of the 2nd mini_batch_Y: (1, 64)\n",
      "shape of the 3rd mini_batch_Y: (1, 64)\n",
      "shape of the 4t mini_batch_Y: (1, 17)\n",
      "mini batch sanity check: [0.60392157 0.69019608 0.50588235]\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size=64\n",
    "mini_batches=random_mini_batches(train_x,train_y,mini_batch_size)\n",
    "print(len(mini_batchs))\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 4th mini_batch_X: \" + str(mini_batches[3][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"shape of the 4t mini_batch_Y: \" + str(mini_batches[3][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-mainstream",
   "metadata": {},
   "source": [
    "<h1>Mini batch model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "thirty-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mini(X,Y,layers_dims, learning_rate,mini_batch_size,num_epochs,print_cost=True):\n",
    "    L=len(layers_dims)\n",
    "    costs=[]\n",
    "    t=0\n",
    "    seed=10\n",
    "    m=X.shape[1]\n",
    "    parameters=initialize_parameters_deep(layers_dims)\n",
    "    for i in range(num_epochs):\n",
    "        seed=seed+1\n",
    "        minibatches=random_mini_batches(X,Y,mini_batch_size,seed)\n",
    "        cost_total=0\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X,minbatch_Y)=minibatch\n",
    "            AL,caches=L_model_forward(minibatch_X,parameters)\n",
    "            cost_total+=compute_cost(AL,minbatch_Y)\n",
    "            grads=L_model_backward(AL,minbatch_Y,caches)\n",
    "            parameters=update_parameters(parameters,grads,learning_rate)\n",
    "        cost_avg=cost_total/m\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "    return parameters        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "national-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.013118\n",
      "Cost after epoch 1000: 0.001951\n",
      "Cost after epoch 2000: 0.001328\n"
     ]
    }
   ],
   "source": [
    "para=model_mini(train_x,train_y,layers_dims,0.01,64,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "economic-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.98 \n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "latest-harvey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.70 \n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, para)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
